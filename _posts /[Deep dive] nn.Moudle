
제목 : [Deep dive] nn.Moudle

우리는 보통 pytorch 로 모델을 만들기 위해서 
1) torch.nn.Module 을 상속 
2) __init__() 과 forward() 를 override 한다. 

그리고 학습/추론 시에 우리는 모델.forward() 와 같이 호출을 해준다. 

(문맥이 이상하니까 이따 수정)

하지만 정확히 nn.Module 이 여기서 어떤일을 하는지 궁금해졌다. 


1) Module 호출 구조 확인하기 

class MLP(nn.Module):
    def __init__(self, d_in, d_hid, d_out):
        super().__init__()
        self.f1 = nn.Linear(d_in, d_hid)
        self.f2 = nn.Linear(d_hid, d_out)

    def forward(self, x):
        x = torch.relu(self.f1(x))
        return self.f2(x)

MLP(x) 

보통 이렇게 model 을 호출한다. 
하지만 우리는 forward() 를 직접호출하지 않고 model(x) 이런식으로 호출한다. ( 여기서 model 은 class name 이다. )
nn.Module 의 코드 구조를 확인해보면

__call__ -> _wrapped_call_impl -> (compiled 이면 그거) -> _call_impl
  ├─ (전/후/역전파 훅 하나도 없으면) forward(...) 바로 호출
  ├─ (있다면)
  │   1) forward_pre_hooks (전역 → 모듈 순)
  │   2) BackwardHook 래퍼 설치 (full/old 후크용)
  │   3) forward(...) 실행
  │   4) forward_hooks (전역 → 모듈 순)
  │   5) backward hook(들) 연결 (non-full은 grad_fn에 hook 등록)
  │
  └─ 예외 발생 시: always_call=True 로 등록된 forward hook은 **예외와 별개로** 호출 시도

이렇게 구성되어 있다. 즉, model(x



