---
layout: single
title: "[Deep dive] nn.Module"
date: 2025-08-15
categories: ["Deepdive"]
tags: ["pytorch", "nn.Module"]
toc: true
author_profile: true
---

![nn.Module Illustration](/assets/images/nn-module.png)

PyTorch로 모델을 만들 때 우리는 보통
1. `torch.nn.Module`을 상속하고,
2. `__init__()`과 `forward()`를 override합니다.
여기서 멈추지 않고, `nn.Module`이 내부에서 정확히 무엇을 해주는지를 구조적으로 정리해보려고 합니다.

1) Module 호출 구조 확인하기

```python
class MLP(nn.Module):
    def __init__(self, d_in, d_hid, d_out):
        super().__init__()
        self.f1 = nn.Linear(d_in, d_hid)
        self.f2 = nn.Linear(d_hid, d_out)

    def forward(self, x):
        x = torch.relu(self.f1(x))
        return self.f2(x)

MLP(x)
```

보통 이렇게 model 을 호출한다. 
하지만 우리는 `forward()` 를 직접호출하지 않고 `model(x)` 이런식으로 호출한다. ( 여기서 `model` 은 class name 이다. )
왜그럴까? `nn.Module` 의 코드 구조를 확인해보면

```
__call__ -> _wrapped_call_impl ->  _call_impl
  ├─ (전/후/역전파 훅 하나도 없으면) forward(...) 바로 호출
  ├─ (있다면)
  │   1) forward_pre_hooks (전역 → 모듈 순)
  │   2) BackwardHook 래퍼 설치 (full/old 후크용)
  │   3) forward(...) 실행
  │   4) forward_hooks (전역 → 모듈 순)
  │   5) backward hook(들) 연결 (non-full은 grad_fn에 hook 등록)
  └─ 예외 발생 시: always_call=True 로 등록된 forward hook은 **예외와 별개로** 호출 시도
```

이렇게 구성되어 있다. 즉, `model(x)` 를 call 하면 자동으로 `forward()` / hook 을 호출하는 것이다. 
(`__call__` 은 클래스의 인스턴스를 함수로 취급하여 인자로 사용할 수 있게 한다.) 그렇기 때문에 PyTorch에서는 `forward()` 를 따로 호출하지 않는다.

그렇다면 **hook** 이란 무엇일까? 
hook 은 forward/backward 시점에 끼어들어 값을 바꾸는 callback function 이다. 
여기서는 네 가지 경우가 있다:

1. forward 전/후
2. backward 전/후

이정도만 알아도 훨씬 Module 에 대한 이해가 잘 되고, 그동안 작성했던 코드에 대해서 더 명료해진거 같다. 

마지막으로 다시 정리해보면 

```
model(X) -> Module.__call__ -> (forward + hook ) -> ( 필요시 backward hook 연결 )
```

로 정리할 수 있겠다.

2) nn.Module 의 내부구조 살펴보기 

이제 nn.Module 이 어떻게 호출되는지, 또 왜 `forward()`시에 직접적으로 호출하지 않아도 되는지 알게 되었다.
다음으로는 nn.Module 에 대해서 조금 더 큰 구조를 확인하고 싶었다. 

모듈이 하는일중에 중요한 것이 우리가 선언한 모델들 안에서 선언한 값들을 자동으로 등록해 주는 것이다. 
모듈은 어떤식으로 등록할까?

먼저 `super().__init__()` 로 아래 레지스트리를 만든다. ( 그래서 이게 선언이 되어 있어야 정상적으로 작동하는 것이다! / 더 정확한 이해는 cf2. 참고 )

- `_parameters` / `_buffers` / `_modules` 딕셔너리
- 각종 훅 dict 등

이를 선언하고 나면 `nn.Module` 에서는 parameter / buffer / modules 를 자동으로 처리하게 되는데, `__setattr__` 를 사용해서 이를 처리한다.

```python
import torch
import torch.nn as nn

class Toy(nn.Module):
    def __init__(self):
        super().__init__()                               # 1) 레지스트리 준비(OrderedDict 등)
        self.w = nn.Parameter(torch.randn(3, 3))         # 2) __setattr__가 가로채서 register_parameter
        self.register_buffer("ctr", torch.zeros(()))     # 3) 버퍼는 이 API로 등록 (권장)
        self.block = nn.Linear(3, 2)                     # 4) 자식 모듈로 자동 등록(add_module)
        self.tmp = torch.tensor(999.)                    # 5) 일반 속성: 등록 안 됨

    def forward(self, x):
        self.ctr = self.ctr + 1                          # 이미 버퍼인 이름 재할당 → 버퍼로 유지
        return self.block(x) @ self.w.t()

m = Toy()
print("params:",   [n for n,_ in m.named_parameters()])  # ['w', 'block.weight', 'block.bias']
print("buffers:",  [n for n,_ in m.named_buffers()])     # ['ctr']
print("state_dict keys:", list(m.state_dict().keys()))   # tmp는 없음
```

이런식으로 사용하고 `__setattr__` 의 코드를 아주 간략하게 보면 
( 참고로 `_modules`, `_parameters` 등은 등록순서가 중요하기 때문에 `OrderedDict` 를 사용한다. )

```python
class Module:
    def __setattr__(self, name, value):
        params  = self.__dict__.get("_parameters")
        modules = self.__dict__.get("_modules")
        buffers = self.__dict__.get("_buffers")

        if isinstance(value, torch.nn.Parameter):
            # 파라미터로 자동 등록
            self.register_parameter(name, value)

        elif params is not None and name in params and value is None:
            # 기존 파라미터 제거/해제
            self.register_parameter(name, None)

        # ...(생략)

        else:  # 그냥 일반 속성 (등록 안 됨)
            super().__setattr__(name, value)
```

`__setattr__` 는 이와 같이 구성되어 있다. 
간단하게 말해서 nn.Module 은 `__setattr__` 를 override 했기 때문에, nn.Module 이 가지고 있는 등록 로직으로 들어온다고 보면된다. 
따라서 속성에 “할당해서” 넣는 서브모듈/파라미터는 전부 `__setattr__`을 거쳐간다(?). 
단, 버퍼는 자동 등록이 안되지만 그냥 아니다~~ 정도만 알고있어도 충분할 듯 싶다. 

마지막으로 이해를 높이기 위해서 `nn.Parameter()` 를 선언 했을때 어떻게 흐름이 이어지는지 정리해보자한다. ( 헷갈릴땐 `__setattr__` 함수를 확인해보자 )

0. `super().__init__()` 호출 → 내부 레지스트리 만듦
1. 코드 내에서 `self.w = nn.Parameter()`
2. `module.__setattr__` 이 (override) 해서 "등록" 하는 로직으로 들어간다 ( 위에 코드처럼 )
3. 어떤 속성인지 확인 → 이것이 parameter인지 확인! → ( 같은 이름의 파라미터가 있다면 제거 )
4. `register_parameter(name, param)` 호출 ( 여기서 각종 검사 진행 )
5. `_parameters` 에 저장

3) hardware - nn.Module 

하드웨어와 nn.Module 과 어떤 관련이 있을까. 마무리에는 역시 하드웨어 만한게 없는거 같다. 진짜 딥다이브하는 느낌이랄까...
다양한 것들이 있지만 이번에는 이쪽이 주 목표가 아니기 때문에 ..!
그래서 이번에는 최근 모델을 학습시킬 일이 있었는데 cache 관련해서 오류가 생겼던 적이 있었는데 ( OOM 이였을까.. ) torch 관련 cache 에 대해서 한번 알아보았다. 

- torch 는 한번 잡은 VRAM 을 cache 에 보관했다가 재사용한다. (그래야 batch 가 빠르다)
- 만약 다른 프로세스에 VRAM 을 양보해야할 때 주로 `empty_cache()` 를 사용한다. → 자주 비우면 속도가 느려진다.
- DataLoader 에서 cache: 미리 batch 를 준비해서 CPU → GPU 전송을 빠르게 해줌 ( 이전 딥다이브 주제 )

당시에 `image.cache`/`label.cache` 이렇게 존재했고. 뭔지 모르겠는데 epoch 을 13정도 하면 항상 늘 거기서 학습이 터져버렸다. 

일단 `image.cache` / `label.cache` 는 VRAM 과 별도라서 문제가 아니였던 것 같기는하다. 그럼 어디서 문제였을까 고민해보면.
입력 크기/배치 문제였을꺼 같은데 이미지가 jpg/png 섞여 있을 뿐만 아니라(이건 문제가 안되는걸로 확인) 해상도가 좀 달랐던 것 같다. ( 더 찾아보니 augmentation 할때 모자이크 시 해상도가 크게 또 바뀐다고 한다.)
이때 torch cache 가 자잘하게 쪼개져서 합계 여유는 있는데 큰 연속 블록이 없어서 OOM 이 난다고 한다.

혹은 DataLoader 에서 GPU 로 너무 배치를 빨리 올려서 GPU 에 너무 많은 데이터가 동시에 쌓여서 터질수 있는데 당시에 `num_workers` 를 1이였나 0이였나 했는데도 터졌던 기억이 있어서 이쪽 문제는 아니였던 것 같다. 
어떻게 해결했냐면? 모자이크를 더 소극적으로 하고 해상도를 맞춰주었다. 

쓰고 나니까 nn.Module 이랑 관련이 없네 .. ㅎㅎ;

cf. )

갑자기 궁금해진게 있다. 보통 이런식으로 학습시키는데

```python
model.train()
for x, y in loader:
    opt.zero_grad()
    y_hat = model(x)          # __call__ -> forward(...) (훅 포함)
    loss = criterion(y_hat, y)
    loss.backward()           # 여기서 그래디언트 계산 시작(모듈이 아님, 'loss 텐서'임)
    opt.step()
```

다시 1) module 호출로 돌아가서. 이런 생각이 들었다. 아니 forward / backward 는 모델이 모르는 거 같은데 backward 시에는 gradient descent 를 위해서 forward 시에 분명 계산값을 저장해둔다고 했는데... 
그럼 그걸 어디서 설정하는거지? 라는 의문이 들었다. 

`nn.Module` 에 `self.training` 태그가 있다고 한다. 그래서 `model.train()` 을 호출하면 `self.training = True` / `model.eval()` 을 부르면 False 가 설정된다고 한다. 
( 마찬가지로 dropout/batchnorm 도 이에 따라서 바뀐다 )

그리고 모델 안에 파라미터 (`nn.Parameter`) 는 앞에서 이야기 했듯이 `requires_grad = True` 로 되어있고 만약 추론시 속도나 메모리 이득을 보기 위해서

```python
model.eval()
with torch.no_grad():   # 또는 torch.inference_mode()  (더 공격적으로 빠름)
    y = model(x)        # 그래프 기록 X
```

다음과 같이 `with torch.no_grad()` 를 설정한다. 

+) 2번 관련해서 
`super().__init__()` 을 안해준다면? 
https://daebaq27.tistory.com/60
